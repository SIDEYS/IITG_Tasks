{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold, LeaveOneGroupOut\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "r40jOvzuFaBn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "s4KdRciwz2J6"
      },
      "outputs": [],
      "source": [
        "X_train = pd.read_csv(\"X_train.txt\", sep='\\s+', header=None)\n",
        "y_train = pd.read_csv(\"y_train.txt\", sep='\\s+', header=None)\n",
        "\n",
        "train_data = pd.concat([X_train, y_train], axis=1)\n",
        "\n",
        "column_names = [f\"Feature_{i+1}\" for i in range(X_train.shape[1])]\n",
        "column_names.append(\"Activity\")\n",
        "train_data.columns = column_names\n",
        "\n",
        "train_data.to_csv(\"train_data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TtE7_qUFmyZ",
        "outputId": "5941d93c-2efc-4ee9-cfbe-29ce89304c28"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7352, 562)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.read_csv(\"X_test.txt\", sep='\\s+', header=None)\n",
        "y_test = pd.read_csv(\"y_test.txt\", sep='\\s+', header=None)\n",
        "\n",
        "test_data = pd.concat([X_test, y_test], axis=1)\n",
        "test_data.columns = column_names\n",
        "test_data.to_csv(\"test_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "aKN_763PFksr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"train_data.csv\")"
      ],
      "metadata": {
        "id": "V8kGNbmDHQGp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(\"test_data.csv\")"
      ],
      "metadata": {
        "id": "7gBJ3Y6mHSC0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_train.drop(columns=[\"Activity\"])\n",
        "y = df_train[\"Activity\"]"
      ],
      "metadata": {
        "id": "s8eT5czaGyh6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XT = df_test.drop(columns=[\"Activity\"])\n",
        "yT = df_test[\"Activity\"]"
      ],
      "metadata": {
        "id": "LXwDqE5oHYxM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33ixT7YgHcBR",
        "outputId": "ad4ef109-de75-4479-87d3-35266e750417"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7352, 561), (7352,))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "XT.shape, yT.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmUvxqv7Hcf5",
        "outputId": "2c602715-d414-4daf-b9a6-942168b08178"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2947, 561), (2947,))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducing Features by removing higly corellated features\n"
      ],
      "metadata": {
        "id": "uA_WqV-5HgtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = X.corr()\n",
        "highly_correlated_features = []\n",
        "\n",
        "for column in correlation_matrix.columns:\n",
        "    correlated_columns = correlation_matrix[column].abs() > 0.85\n",
        "    correlated_columns[column] = False\n",
        "\n",
        "    if correlated_columns.any():\n",
        "        highly_correlated_features.append(column)\n",
        "\n",
        "print(\"Highly correlated features:\", highly_correlated_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyNXShGpHejR",
        "outputId": "c259dfdc-f6d5-48fd-f2e7-4677d46ec7d4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highly correlated features: ['Feature_4', 'Feature_5', 'Feature_6', 'Feature_7', 'Feature_8', 'Feature_9', 'Feature_10', 'Feature_11', 'Feature_12', 'Feature_13', 'Feature_14', 'Feature_15', 'Feature_16', 'Feature_17', 'Feature_18', 'Feature_19', 'Feature_20', 'Feature_21', 'Feature_22', 'Feature_23', 'Feature_24', 'Feature_25', 'Feature_26', 'Feature_27', 'Feature_30', 'Feature_31', 'Feature_34', 'Feature_35', 'Feature_41', 'Feature_42', 'Feature_43', 'Feature_44', 'Feature_45', 'Feature_46', 'Feature_47', 'Feature_48', 'Feature_49', 'Feature_50', 'Feature_51', 'Feature_52', 'Feature_53', 'Feature_54', 'Feature_55', 'Feature_57', 'Feature_58', 'Feature_60', 'Feature_61', 'Feature_62', 'Feature_66', 'Feature_67', 'Feature_68', 'Feature_69', 'Feature_70', 'Feature_71', 'Feature_72', 'Feature_73', 'Feature_74', 'Feature_75', 'Feature_76', 'Feature_77', 'Feature_84', 'Feature_85', 'Feature_86', 'Feature_87', 'Feature_88', 'Feature_89', 'Feature_90', 'Feature_91', 'Feature_92', 'Feature_93', 'Feature_94', 'Feature_95', 'Feature_96', 'Feature_97', 'Feature_98', 'Feature_99', 'Feature_100', 'Feature_101', 'Feature_102', 'Feature_103', 'Feature_104', 'Feature_105', 'Feature_106', 'Feature_110', 'Feature_114', 'Feature_124', 'Feature_125', 'Feature_126', 'Feature_127', 'Feature_128', 'Feature_129', 'Feature_130', 'Feature_131', 'Feature_132', 'Feature_133', 'Feature_134', 'Feature_135', 'Feature_136', 'Feature_137', 'Feature_138', 'Feature_139', 'Feature_140', 'Feature_141', 'Feature_142', 'Feature_146', 'Feature_147', 'Feature_150', 'Feature_151', 'Feature_154', 'Feature_155', 'Feature_156', 'Feature_164', 'Feature_165', 'Feature_166', 'Feature_167', 'Feature_168', 'Feature_169', 'Feature_170', 'Feature_171', 'Feature_172', 'Feature_173', 'Feature_174', 'Feature_175', 'Feature_176', 'Feature_177', 'Feature_178', 'Feature_179', 'Feature_180', 'Feature_181', 'Feature_182', 'Feature_183', 'Feature_184', 'Feature_185', 'Feature_186', 'Feature_190', 'Feature_194', 'Feature_195', 'Feature_201', 'Feature_202', 'Feature_203', 'Feature_204', 'Feature_205', 'Feature_206', 'Feature_207', 'Feature_208', 'Feature_209', 'Feature_210', 'Feature_211', 'Feature_212', 'Feature_213', 'Feature_214', 'Feature_215', 'Feature_216', 'Feature_217', 'Feature_218', 'Feature_219', 'Feature_220', 'Feature_221', 'Feature_222', 'Feature_223', 'Feature_224', 'Feature_225', 'Feature_226', 'Feature_227', 'Feature_228', 'Feature_229', 'Feature_230', 'Feature_231', 'Feature_232', 'Feature_233', 'Feature_234', 'Feature_235', 'Feature_240', 'Feature_241', 'Feature_242', 'Feature_243', 'Feature_245', 'Feature_246', 'Feature_247', 'Feature_249', 'Feature_250', 'Feature_253', 'Feature_254', 'Feature_255', 'Feature_256', 'Feature_257', 'Feature_258', 'Feature_259', 'Feature_260', 'Feature_261', 'Feature_266', 'Feature_267', 'Feature_268', 'Feature_269', 'Feature_270', 'Feature_271', 'Feature_272', 'Feature_273', 'Feature_274', 'Feature_275', 'Feature_276', 'Feature_277', 'Feature_281', 'Feature_282', 'Feature_283', 'Feature_284', 'Feature_285', 'Feature_286', 'Feature_287', 'Feature_288', 'Feature_289', 'Feature_290', 'Feature_297', 'Feature_298', 'Feature_299', 'Feature_300', 'Feature_301', 'Feature_302', 'Feature_303', 'Feature_304', 'Feature_305', 'Feature_306', 'Feature_307', 'Feature_308', 'Feature_309', 'Feature_310', 'Feature_311', 'Feature_312', 'Feature_313', 'Feature_314', 'Feature_315', 'Feature_316', 'Feature_317', 'Feature_318', 'Feature_319', 'Feature_320', 'Feature_321', 'Feature_322', 'Feature_323', 'Feature_324', 'Feature_325', 'Feature_326', 'Feature_327', 'Feature_328', 'Feature_329', 'Feature_330', 'Feature_331', 'Feature_332', 'Feature_333', 'Feature_334', 'Feature_335', 'Feature_336', 'Feature_337', 'Feature_338', 'Feature_339', 'Feature_340', 'Feature_341', 'Feature_342', 'Feature_343', 'Feature_344', 'Feature_345', 'Feature_346', 'Feature_347', 'Feature_348', 'Feature_349', 'Feature_350', 'Feature_351', 'Feature_352', 'Feature_353', 'Feature_354', 'Feature_355', 'Feature_356', 'Feature_360', 'Feature_361', 'Feature_362', 'Feature_363', 'Feature_364', 'Feature_365', 'Feature_366', 'Feature_367', 'Feature_368', 'Feature_369', 'Feature_373', 'Feature_374', 'Feature_375', 'Feature_376', 'Feature_377', 'Feature_378', 'Feature_379', 'Feature_380', 'Feature_381', 'Feature_382', 'Feature_383', 'Feature_384', 'Feature_385', 'Feature_386', 'Feature_387', 'Feature_388', 'Feature_390', 'Feature_391', 'Feature_392', 'Feature_393', 'Feature_394', 'Feature_395', 'Feature_396', 'Feature_397', 'Feature_398', 'Feature_399', 'Feature_400', 'Feature_401', 'Feature_402', 'Feature_404', 'Feature_405', 'Feature_406', 'Feature_407', 'Feature_408', 'Feature_409', 'Feature_410', 'Feature_411', 'Feature_412', 'Feature_413', 'Feature_414', 'Feature_415', 'Feature_416', 'Feature_418', 'Feature_419', 'Feature_420', 'Feature_421', 'Feature_422', 'Feature_423', 'Feature_424', 'Feature_425', 'Feature_426', 'Feature_427', 'Feature_428', 'Feature_429', 'Feature_430', 'Feature_431', 'Feature_432', 'Feature_433', 'Feature_434', 'Feature_435', 'Feature_439', 'Feature_440', 'Feature_441', 'Feature_442', 'Feature_443', 'Feature_444', 'Feature_445', 'Feature_446', 'Feature_447', 'Feature_448', 'Feature_455', 'Feature_456', 'Feature_457', 'Feature_458', 'Feature_459', 'Feature_460', 'Feature_461', 'Feature_462', 'Feature_463', 'Feature_464', 'Feature_465', 'Feature_466', 'Feature_467', 'Feature_468', 'Feature_469', 'Feature_470', 'Feature_471', 'Feature_472', 'Feature_473', 'Feature_474', 'Feature_475', 'Feature_477', 'Feature_478', 'Feature_479', 'Feature_480', 'Feature_481', 'Feature_482', 'Feature_483', 'Feature_484', 'Feature_485', 'Feature_486', 'Feature_487', 'Feature_488', 'Feature_489', 'Feature_490', 'Feature_491', 'Feature_492', 'Feature_493', 'Feature_494', 'Feature_495', 'Feature_496', 'Feature_497', 'Feature_498', 'Feature_499', 'Feature_500', 'Feature_501', 'Feature_502', 'Feature_503', 'Feature_504', 'Feature_505', 'Feature_506', 'Feature_508', 'Feature_509', 'Feature_510', 'Feature_511', 'Feature_514', 'Feature_515', 'Feature_516', 'Feature_517', 'Feature_518', 'Feature_519', 'Feature_521', 'Feature_522', 'Feature_523', 'Feature_524', 'Feature_527', 'Feature_528', 'Feature_529', 'Feature_530', 'Feature_531', 'Feature_532', 'Feature_534', 'Feature_535', 'Feature_536', 'Feature_537', 'Feature_540', 'Feature_541', 'Feature_542', 'Feature_543', 'Feature_544', 'Feature_545', 'Feature_547', 'Feature_548', 'Feature_549', 'Feature_550', 'Feature_553', 'Feature_554', 'Feature_559', 'Feature_560', 'Feature_561']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(highly_correlated_features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5YXAGskIcyp",
        "outputId": "2b7751f2-afeb-457a-e6ac-c8f97593ad1a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Droping the highly corelated features as they are redundant and are analyzed whike going thorugh other features"
      ],
      "metadata": {
        "id": "_lD--0bZJoSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.drop(highly_correlated_features, axis=1, inplace=True)\n",
        "XT.drop(highly_correlated_features, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "NGYiZ_6EIfDN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=500),\n",
        "    'AdaBoost': AdaBoostClassifier(algorithm=\"SAMME\")\n",
        "}"
      ],
      "metadata": {
        "id": "qzPsztxbJeVE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that X contains only those features that are not corellated to others."
      ],
      "metadata": {
        "id": "fWt9LHrQLr1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYgCDvWrLnrn",
        "outputId": "bfb21eca-2dae-4f19-a185-97f3662c8399"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Feature_1  Feature_2  Feature_3  Feature_28  Feature_29  Feature_32  \\\n",
            "0      0.288585  -0.020294  -0.132905    0.359910   -0.058526    0.264106   \n",
            "1      0.278419  -0.016411  -0.123520    0.284213    0.284595    0.294310   \n",
            "2      0.279653  -0.019467  -0.113462    0.337202   -0.164739    0.342256   \n",
            "3      0.279174  -0.026201  -0.123283    0.198204   -0.264307    0.323154   \n",
            "4      0.276629  -0.016570  -0.115362    0.191161    0.086904    0.434728   \n",
            "...         ...        ...        ...         ...         ...         ...   \n",
            "7347   0.299665  -0.057193  -0.181233   -0.078255   -0.056751   -0.119821   \n",
            "7348   0.273853  -0.007749  -0.147468    0.206839   -0.154722    0.034260   \n",
            "7349   0.273387  -0.017011  -0.045022    0.063584   -0.017019    0.119962   \n",
            "7350   0.289654  -0.018843  -0.158281    0.009588   -0.038354    0.101761   \n",
            "7351   0.351503  -0.012423  -0.203867   -0.230562    0.139282   -0.156435   \n",
            "\n",
            "      Feature_33  Feature_36  Feature_37  Feature_38  ...  Feature_533  \\\n",
            "0      -0.095246    0.491936   -0.190884    0.376314  ...    -0.989498   \n",
            "1      -0.281211   -0.016657   -0.220643   -0.013429  ...    -0.991829   \n",
            "2      -0.332564    0.173863   -0.299493   -0.124698  ...    -0.995703   \n",
            "3      -0.170813    0.482148   -0.470129   -0.305693  ...    -0.996199   \n",
            "4      -0.315375    0.179414   -0.088952   -0.155804  ...    -0.998353   \n",
            "...          ...         ...         ...         ...  ...          ...   \n",
            "7347    0.293112   -0.205315    0.142117   -0.211822  ...    -0.995117   \n",
            "7348    0.239835    0.188717   -0.207505   -0.198555  ...    -0.916112   \n",
            "7349    0.080689   -0.033780    0.016677   -0.226826  ...    -0.972099   \n",
            "7350   -0.108375   -0.234309    0.232444   -0.257775  ...    -0.959329   \n",
            "7351    0.097870   -0.056556    0.054368   -0.266442  ...    -0.779153   \n",
            "\n",
            "      Feature_538  Feature_539  Feature_546  Feature_551  Feature_552  \\\n",
            "0       -1.000000    -0.128989    -0.991048    -1.000000    -0.074323   \n",
            "1       -0.948718    -0.271958    -0.994440    -1.000000     0.158075   \n",
            "2       -0.794872    -0.212728    -0.995866    -0.555556     0.414503   \n",
            "3       -1.000000    -0.035684    -0.995732    -0.936508     0.404573   \n",
            "4       -0.897436    -0.273582    -0.997418    -0.936508     0.087753   \n",
            "...           ...          ...          ...          ...          ...   \n",
            "7347    -0.897436    -0.376234    -0.925241    -0.904762    -0.070157   \n",
            "7348    -0.846154    -0.296176    -0.761880    -0.904762     0.165259   \n",
            "7349    -0.846154    -0.320249    -0.692582    -0.904762     0.195034   \n",
            "7350    -0.846154    -0.412332    -0.886154    -0.904762     0.013865   \n",
            "7351    -0.846154    -0.389084    -0.758685    -0.904762    -0.058402   \n",
            "\n",
            "      Feature_555  Feature_556  Feature_557  Feature_558  \n",
            "0       -0.112754     0.030400    -0.464761    -0.018446  \n",
            "1        0.053477    -0.007435    -0.732626     0.703511  \n",
            "2       -0.118559     0.177899     0.100699     0.808529  \n",
            "3       -0.036788    -0.012892     0.640011    -0.485366  \n",
            "4        0.123320     0.122542     0.693578    -0.615971  \n",
            "...           ...          ...          ...          ...  \n",
            "7347    -0.190437     0.829718     0.206972    -0.425619  \n",
            "7348     0.064907     0.875679    -0.879033     0.400219  \n",
            "7349     0.052806    -0.266724     0.864404     0.701169  \n",
            "7350    -0.101360     0.700740     0.936674    -0.589479  \n",
            "7351    -0.280088    -0.007739    -0.056088    -0.616956  \n",
            "\n",
            "[7352 rows x 119 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code performs K-Fold Cross Validation (with 5 splits) on the specified models to evaluate their performance with reduced features. It calculates and prints the mean accuracy, precision, recall and F1 for each model using the training data"
      ],
      "metadata": {
        "id": "H1HW3E3lK_Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'precision': make_scorer(precision_score, average='weighted'),\n",
        "    'recall': make_scorer(recall_score, average='weighted'),\n",
        "    'f1': make_scorer(f1_score, average='weighted')\n",
        "}\n",
        "\n",
        "print(\"\\nModel Performance (K-Fold Cross Validation):\")\n",
        "for model_name, model in models.items():\n",
        "    scores = cross_validate(model, X, y, cv=kfold, scoring=scoring)\n",
        "    print(f\"{model_name} - Accuracy: {scores['test_accuracy'].mean():.4f} ± {scores['test_accuracy'].std():.4f}\")\n",
        "    print(f\"{model_name} - Precision (Weighted): {scores['test_precision'].mean():.4f} ± {scores['test_precision'].std():.4f}\")\n",
        "    print(f\"{model_name} - Recall (Weighted): {scores['test_recall'].mean():.4f} ± {scores['test_recall'].std():.4f}\")\n",
        "    print(f\"{model_name} - F1 Score (Weighted): {scores['test_f1'].mean():.4f} ± {scores['test_f1'].std():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fopi2EzJmdE",
        "outputId": "dfd7d6c2-c2d6-4697-ec07-ba0ffdba83fd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance (K-Fold Cross Validation):\n",
            "Random Forest - Accuracy: 0.9410 ± 0.0019\n",
            "Random Forest - Precision (Weighted): 0.9417 ± 0.0020\n",
            "Random Forest - Recall (Weighted): 0.9410 ± 0.0019\n",
            "Random Forest - F1 Score (Weighted): 0.9411 ± 0.0019\n",
            "Decision Tree - Accuracy: 0.8473 ± 0.0093\n",
            "Decision Tree - Precision (Weighted): 0.8480 ± 0.0090\n",
            "Decision Tree - Recall (Weighted): 0.8473 ± 0.0093\n",
            "Decision Tree - F1 Score (Weighted): 0.8473 ± 0.0093\n",
            "Logistic Regression - Accuracy: 0.9286 ± 0.0022\n",
            "Logistic Regression - Precision (Weighted): 0.9289 ± 0.0026\n",
            "Logistic Regression - Recall (Weighted): 0.9286 ± 0.0022\n",
            "Logistic Regression - F1 Score (Weighted): 0.9286 ± 0.0024\n",
            "AdaBoost - Accuracy: 0.6027 ± 0.0085\n",
            "AdaBoost - Precision (Weighted): 0.6087 ± 0.0254\n",
            "AdaBoost - Recall (Weighted): 0.6027 ± 0.0085\n",
            "AdaBoost - F1 Score (Weighted): 0.5888 ± 0.0203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(XT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BqvvUZ3L7ko",
        "outputId": "9254ce06-9d01-491a-9af9-a36caeaafcb8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Feature_1  Feature_2  Feature_3  Feature_28  Feature_29  Feature_32  \\\n",
            "0      0.257178  -0.023285  -0.014654    0.482280   -0.045462    0.130858   \n",
            "1      0.286027  -0.013163  -0.119083    0.040674    0.272991    0.411411   \n",
            "2      0.275485  -0.026050  -0.118152    0.032703    0.192385    0.470819   \n",
            "3      0.270298  -0.032614  -0.117520    0.034200    0.153639    0.446100   \n",
            "4      0.274833  -0.027848  -0.129527   -0.032804    0.294340    0.168419   \n",
            "...         ...        ...        ...         ...         ...         ...   \n",
            "2942   0.310155  -0.053391  -0.099109    0.114556    0.069925    0.013961   \n",
            "2943   0.363385  -0.039214  -0.105915    0.124019    0.133963    0.070093   \n",
            "2944   0.349966   0.030077  -0.115788    0.400424   -0.123618    0.170446   \n",
            "2945   0.237594   0.018467  -0.096499    0.541771   -0.204716    0.233641   \n",
            "2946   0.153627  -0.018437  -0.137018    0.406554   -0.151102    0.338025   \n",
            "\n",
            "      Feature_33  Feature_36  Feature_37  Feature_38  ...  Feature_533  \\\n",
            "0      -0.014176   -0.171516    0.040063    0.076989  ...    -0.937959   \n",
            "1      -0.340466    0.035305   -0.010083   -0.104983  ...    -0.991121   \n",
            "2      -0.507395    0.632120   -0.550708    0.305653  ...    -0.997663   \n",
            "3      -0.419496    0.416376   -0.286445   -0.063792  ...    -0.996129   \n",
            "4      -0.068156   -0.145931   -0.050197    0.235151  ...    -0.996970   \n",
            "...          ...         ...         ...         ...  ...          ...   \n",
            "2942    0.163305   -0.467399    0.117754   -0.258908  ...    -0.861894   \n",
            "2943    0.085764   -0.225698   -0.039828   -0.249325  ...    -0.682096   \n",
            "2944    0.047362    0.291691   -0.347075   -0.351080  ...    -0.807723   \n",
            "2945    0.047228   -0.054894   -0.038834   -0.190791  ...    -0.918150   \n",
            "2946   -0.123040   -0.001256   -0.179224   -0.080295  ...    -0.941227   \n",
            "\n",
            "      Feature_538  Feature_539  Feature_546  Feature_551  Feature_552  \\\n",
            "0       -1.000000    -0.047391    -0.909829    -1.000000     0.071645   \n",
            "1       -1.000000    -0.031474    -0.986902    -1.000000    -0.401189   \n",
            "2       -0.897436    -0.168805    -0.989609    -0.936508     0.062891   \n",
            "3       -0.948718    -0.285636    -0.992021    -0.936508     0.116695   \n",
            "4       -1.000000    -0.349061    -0.992505    -0.936508    -0.121711   \n",
            "...           ...          ...          ...          ...          ...   \n",
            "2942    -0.897436     0.093839    -0.770173    -0.904762     0.074472   \n",
            "2943    -1.000000     0.092162    -0.947627    -0.904762     0.101859   \n",
            "2944    -0.846154    -0.122065    -0.930426    -0.904762    -0.066249   \n",
            "2945    -0.846154    -0.220897    -0.785712    -0.904762    -0.046467   \n",
            "2946    -0.846154     0.037595    -0.852277    -0.904762    -0.010386   \n",
            "\n",
            "      Feature_555  Feature_556  Feature_557  Feature_558  \n",
            "0        0.006462     0.162920    -0.825886     0.271151  \n",
            "1       -0.083495     0.017500    -0.434375     0.920593  \n",
            "2       -0.034956     0.202302     0.064103     0.145068  \n",
            "3       -0.017067     0.154438     0.340134     0.296407  \n",
            "4       -0.002223    -0.040046     0.736715    -0.118545  \n",
            "...           ...          ...          ...          ...  \n",
            "2942    -0.337422     0.346295     0.884904    -0.698885  \n",
            "2943    -0.736701    -0.372889    -0.657421     0.322549  \n",
            "2944    -0.181560     0.088574     0.696663     0.363139  \n",
            "2945     0.444558    -0.819188     0.929294    -0.008398  \n",
            "2946     0.598808    -0.287951     0.876030    -0.024965  \n",
            "\n",
            "[2947 rows x 119 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code performs Leave-One-Group-Out Cross Validation (LOGO CV) using the provided group labels (subject_train.txt) to evaluate model performance using reduced features. It calculates and prints the mean accuracy and standard deviation for each model using the training data"
      ],
      "metadata": {
        "id": "_E0tZpOLLw8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_labels = np.loadtxt('subject_train.txt')\n",
        "logo = LeaveOneGroupOut()\n",
        "\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'precision': make_scorer(precision_score, average='weighted'),\n",
        "    'recall': make_scorer(recall_score, average='weighted'),\n",
        "    'f1': make_scorer(f1_score, average='weighted')\n",
        "}\n",
        "\n",
        "print(\"\\nModel Performance (Leave-One-Subject-Out CV):\")\n",
        "for model_name, model in models.items():\n",
        "    scores = cross_validate(model, X, y, groups=group_labels, cv=logo, scoring=scoring)\n",
        "    print(f\"{model_name} - Accuracy: {scores['test_accuracy'].mean():.4f} ± {scores['test_accuracy'].std():.4f}\")\n",
        "    print(f\"{model_name} - Precision (Weighted): {scores['test_precision'].mean():.4f} ± {scores['test_precision'].std():.4f}\")\n",
        "    print(f\"{model_name} - Recall (Weighted): {scores['test_recall'].mean():.4f} ± {scores['test_recall'].std():.4f}\")\n",
        "    print(f\"{model_name} - F1 Score (Weighted): {scores['test_f1'].mean():.4f} ± {scores['test_f1'].std():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_DpO4RHLiAm",
        "outputId": "4ee9b2c6-c752-4a78-a298-34a53b46a073"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance (Leave-One-Subject-Out CV):\n",
            "Random Forest - Accuracy: 0.8676 ± 0.0928\n",
            "Random Forest - Precision (Weighted): 0.8934 ± 0.0695\n",
            "Random Forest - Recall (Weighted): 0.8676 ± 0.0928\n",
            "Random Forest - F1 Score (Weighted): 0.8592 ± 0.1032\n",
            "Decision Tree - Accuracy: 0.7509 ± 0.0960\n",
            "Decision Tree - Precision (Weighted): 0.7688 ± 0.0850\n",
            "Decision Tree - Recall (Weighted): 0.7509 ± 0.0960\n",
            "Decision Tree - F1 Score (Weighted): 0.7431 ± 0.1012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression - Accuracy: 0.8734 ± 0.0806\n",
            "Logistic Regression - Precision (Weighted): 0.8869 ± 0.0769\n",
            "Logistic Regression - Recall (Weighted): 0.8734 ± 0.0806\n",
            "Logistic Regression - F1 Score (Weighted): 0.8674 ± 0.0896\n",
            "AdaBoost - Accuracy: 0.5876 ± 0.0936\n",
            "AdaBoost - Precision (Weighted): 0.6020 ± 0.0873\n",
            "AdaBoost - Recall (Weighted): 0.5876 ± 0.0936\n",
            "AdaBoost - F1 Score (Weighted): 0.5656 ± 0.0960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Test data"
      ],
      "metadata": {
        "id": "lLKlk_BhNamE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'precision': make_scorer(precision_score, average='weighted'),\n",
        "    'recall': make_scorer(recall_score, average='weighted'),\n",
        "    'f1': make_scorer(f1_score, average='weighted')\n",
        "}\n",
        "\n",
        "print(\"\\nModel Performance (K-Fold Cross Validation):\")\n",
        "for model_name, model in models.items():\n",
        "    scores = cross_validate(model, XT, yT, cv=kfold, scoring=scoring)\n",
        "    print(f\"{model_name} - Accuracy: {scores['test_accuracy'].mean():.4f} ± {scores['test_accuracy'].std():.4f}\")\n",
        "    print(f\"{model_name} - Precision (Weighted): {scores['test_precision'].mean():.4f} ± {scores['test_precision'].std():.4f}\")\n",
        "    print(f\"{model_name} - Recall (Weighted): {scores['test_recall'].mean():.4f} ± {scores['test_recall'].std():.4f}\")\n",
        "    print(f\"{model_name} - F1 Score (Weighted): {scores['test_f1'].mean():.4f} ± {scores['test_f1'].std():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHmBl5PmLH88",
        "outputId": "87ed9c1c-961e-41ba-fe1f-05d7d46ce057"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance (K-Fold Cross Validation):\n",
            "Random Forest - Accuracy: 0.9515 ± 0.0079\n",
            "Random Forest - Precision (Weighted): 0.9521 ± 0.0079\n",
            "Random Forest - Recall (Weighted): 0.9515 ± 0.0079\n",
            "Random Forest - F1 Score (Weighted): 0.9513 ± 0.0080\n",
            "Decision Tree - Accuracy: 0.8442 ± 0.0151\n",
            "Decision Tree - Precision (Weighted): 0.8452 ± 0.0150\n",
            "Decision Tree - Recall (Weighted): 0.8442 ± 0.0151\n",
            "Decision Tree - F1 Score (Weighted): 0.8440 ± 0.0153\n",
            "Logistic Regression - Accuracy: 0.9379 ± 0.0099\n",
            "Logistic Regression - Precision (Weighted): 0.9388 ± 0.0098\n",
            "Logistic Regression - Recall (Weighted): 0.9379 ± 0.0099\n",
            "Logistic Regression - F1 Score (Weighted): 0.9380 ± 0.0099\n",
            "AdaBoost - Accuracy: 0.5277 ± 0.0102\n",
            "AdaBoost - Precision (Weighted): 0.5637 ± 0.0141\n",
            "AdaBoost - Recall (Weighted): 0.5277 ± 0.0102\n",
            "AdaBoost - F1 Score (Weighted): 0.5142 ± 0.0087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "group_labels = np.loadtxt('subject_test.txt')\n",
        "logo = LeaveOneGroupOut()\n",
        "\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'precision': make_scorer(precision_score, average='weighted'),\n",
        "    'recall': make_scorer(recall_score, average='weighted'),\n",
        "    'f1': make_scorer(f1_score, average='weighted')\n",
        "}\n",
        "\n",
        "print(\"\\nModel Performance (Leave-One-Subject-Out CV):\")\n",
        "for model_name, model in models.items():\n",
        "    scores = cross_validate(model, XT, yT, groups=group_labels, cv=logo, scoring=scoring)\n",
        "    print(f\"{model_name} - Accuracy: {scores['test_accuracy'].mean():.4f} ± {scores['test_accuracy'].std():.4f}\")\n",
        "    print(f\"{model_name} - Precision (Weighted): {scores['test_precision'].mean():.4f} ± {scores['test_precision'].std():.4f}\")\n",
        "    print(f\"{model_name} - Recall (Weighted): {scores['test_recall'].mean():.4f} ± {scores['test_recall'].std():.4f}\")\n",
        "    print(f\"{model_name} - F1 Score (Weighted): {scores['test_f1'].mean():.4f} ± {scores['test_f1'].std():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EuLuH9GOJ35",
        "outputId": "5457ed2e-5e97-4483-9d95-2cd59a490026"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance (Leave-One-Subject-Out CV):\n",
            "Random Forest - Accuracy: 0.8661 ± 0.0437\n",
            "Random Forest - Precision (Weighted): 0.8809 ± 0.0352\n",
            "Random Forest - Recall (Weighted): 0.8661 ± 0.0437\n",
            "Random Forest - F1 Score (Weighted): 0.8618 ± 0.0479\n",
            "Decision Tree - Accuracy: 0.6983 ± 0.0642\n",
            "Decision Tree - Precision (Weighted): 0.7106 ± 0.0645\n",
            "Decision Tree - Recall (Weighted): 0.6983 ± 0.0642\n",
            "Decision Tree - F1 Score (Weighted): 0.6938 ± 0.0635\n",
            "Logistic Regression - Accuracy: 0.8338 ± 0.0896\n",
            "Logistic Regression - Precision (Weighted): 0.8566 ± 0.0816\n",
            "Logistic Regression - Recall (Weighted): 0.8338 ± 0.0896\n",
            "Logistic Regression - F1 Score (Weighted): 0.8255 ± 0.0995\n",
            "AdaBoost - Accuracy: 0.5044 ± 0.0445\n",
            "AdaBoost - Precision (Weighted): 0.5374 ± 0.0416\n",
            "AdaBoost - Recall (Weighted): 0.5044 ± 0.0445\n",
            "AdaBoost - F1 Score (Weighted): 0.4916 ± 0.0451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nModel Performance on Test Data:\")\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X, y)\n",
        "    y_pred = model.predict(XT)\n",
        "    acc = accuracy_score(yT, y_pred)\n",
        "    prec = precision_score(yT, y_pred, average='weighted')\n",
        "    rec = recall_score(yT, y_pred, average='weighted')\n",
        "    f1 = f1_score(yT, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"{model_name} -> Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWVALtPEOeEU",
        "outputId": "fa9220d5-79cc-49b3-a631-4b2bada0793b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance on Test Data:\n",
            "Random Forest -> Accuracy: 0.8962, Precision: 0.8996, Recall: 0.8962, F1 Score: 0.8961\n",
            "Decision Tree -> Accuracy: 0.7615, Precision: 0.7692, Recall: 0.7615, F1 Score: 0.7613\n",
            "Logistic Regression -> Accuracy: 0.8812, Precision: 0.8843, Recall: 0.8812, F1 Score: 0.8817\n",
            "AdaBoost -> Accuracy: 0.5385, Precision: 0.6002, Recall: 0.5385, F1 Score: 0.5102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion:- The accuracies for all the models across train and test data have decreased except for adaboost classifier which showed a significant increase in the model accuracy and achieved an max accuracy score of 60.27% using K-fold evaluation on train data. The reduce of accuracy may indicate that feature selection can be done better as the features after dropping the higly corellated features do not entirely represent the entire dataset and thus a drop in accuracy score. Additionally, we can see that some models like adaboost claddifier are sensitive to the dimensionality of the data and thus highlight the importance of dimensionality reduction."
      ],
      "metadata": {
        "id": "6KwEES7ITRTd"
      }
    }
  ]
}